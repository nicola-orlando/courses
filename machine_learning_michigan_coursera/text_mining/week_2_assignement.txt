# Question 8 is incorrect 

Assignment 2 - Introduction to NLTK
In part 1 of this assignment you will use nltk to explore the Herman Melville novel Moby Dick. Then in part 2 you will create a spelling recommender function that uses nltk to find words similar to the misspelling.
Part 1 - Analyzing Moby Dick

import nltk
import pandas as pd
import numpy as np
nltk.download('punkt')

# If you would like to work with the raw text you can use 'moby_raw'
with open('moby.txt', 'r') as f:
    moby_raw = f.read()
    
# If you would like to work with the novel in nltk.Text format you can use 'text1'
moby_tokens = nltk.word_tokenize(moby_raw)
text1 = nltk.Text(moby_tokens)

Example 1
How many tokens (words and punctuation symbols) are in text1?
This function should return an integer.

def example_one():
    
    return len(nltk.word_tokenize(moby_raw)) # or alternatively len(text1)

example_one()

Example 2
How many unique tokens (unique words and punctuation) does text1 have?
This function should return an integer.

def example_two():
    
    return len(set(nltk.word_tokenize(moby_raw))) # or alternatively len(set(text1))

example_two()

Example 3
After lemmatizing the verbs, how many unique tokens does text1 have?
This function should return an integer.

import nltk
nltk.download('wordnet')
    
from nltk.stem import WordNetLemmatizer

def example_three():

    lemmatizer = WordNetLemmatizer()
    lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]

    return len(set(lemmatized))

example_three()


Question 1
What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)
This function should return a float.

def answer_one():
    
    unique_tokens = len(set(nltk.word_tokenize(moby_raw))) 
    tokens = len(nltk.word_tokenize(moby_raw))
    output = unique_tokens / tokens
    return output # Your answer here

answer_one()


Question 2
What percentage of tokens is 'whale'or 'Whale'?
This function should return a float.

def answer_two():
    
    whales = [ w for w in nltk.word_tokenize(moby_raw) if(w=='whale' or w=='Whale')]
    tokens = len(set(nltk.word_tokenize(moby_raw)))
    output = len(whales) / tokens
    return output # Your answer here

answer_two()


Question 3
What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?
This function should return a list of 20 tuples where each tuple is of the form (token, frequency). The list should be sorted in descending order of frequency.

from nltk.probability import FreqDist

def answer_three():
    # Get unique tokens 
    unique_tokens = set(nltk.word_tokenize(moby_raw)) 
    tokens = nltk.word_tokenize(moby_raw)
    fdist = FreqDist(tokens)
    output = fdist.most_common(20) 
    return output # Your answer here

answer_three()


Question 4
What tokens have a length of greater than 5 and frequency of more than 150?
This function should return an alphabetically sorted list of the tokens that match the above constraints. To sort your list, use sorted()

def answer_four():
    
    unique_tokens = set(nltk.word_tokenize(moby_raw)) 
    tokens = nltk.word_tokenize(moby_raw)
    fdist = FreqDist(tokens)
    input_tuple = fdist.most_common(500)
    output = []
    for element in input_tuple: 
        if element[1]>150 and len(element[0])>5: 
            output.append(element[0])
    output_sorted = sorted(output)
    return output_sorted # Your answer here

answer_four()


Question 5
Find the longest word in text1 and that word's length.
This function should return a tuple (longest_word, length).

def answer_five():
    
    unique_tokens = set(nltk.word_tokenize(moby_raw)) 
    longest_word = max(unique_tokens, key=len)
    output = (longest_word, len(longest_word))
    return output # Your answer here

answer_five()


Question 6
What unique words have a frequency of more than 2000? What is their frequency?
"Hint: you may want to use isalpha() to check if the token is a word and not punctuation."
This function should return a list of tuples of the form (frequency, word) sorted in descending order of frequency.

def answer_six():
            
    tokens = nltk.word_tokenize(moby_raw)
    fdist = FreqDist(tokens)
    
    input_tuple = fdist.most_common(100)
    output = []
    for element in input_tuple: 
        if element[1]>=2000 and element[0].isalpha(): 
            output.append((element[1],element[0]))

    return output # Your answer here

answer_six()


Question 7
What is the average number of tokens per sentence?
This function should return a float.

def answer_seven():
    
    sent_text = nltk.sent_tokenize(moby_raw)
    total_tokens = 0 

    for element in sent_text: 
        total_tokens = total_tokens + len(nltk.word_tokenize(element))
                
    output = total_tokens / len(sent_text) 
    return output # Your answer here

answer_seven()


Question 8
What are the 5 most frequent parts of speech in this text? What is their frequency?
This function should return a list of tuples of the form (part_of_speech, frequency) sorted in descending order of frequency.


import nltk
nltk.download('averaged_perceptron_tagger')

def answer_eight():
    
    pos_list = nltk.pos_tag(moby_raw)
    #print(type(pos_list))
    pos_counts = nltk.FreqDist(tag for (word, tag) in pos_list)
    output = pos_counts.most_common(5)
    return output # Your answer here

answer_eight()


Part 2 - Spelling Recommender
For this part of the assignment you will create three different spelling recommenders, that each take a list of misspelled words and recommends a correctly spelled word for every word in the list.
For every misspelled word, the recommender should find find the word in correct_spellings that has the shortest distance*, and starts with the same letter as the misspelled word, and return that word as a recommendation.
*Each of the three different recommenders will use a different distance measure (outlined below).
Each of the recommenders should provide recommendations for the three default words provided: ['cormulent', 'incendenece', 'validrate'].


from nltk.corpus import words
nltk.download('words')

correct_spellings = words.words()

Question 9
For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:
Jaccard distance on the trigrams of the two words.
This function should return a list of length three: ['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation'].

from nltk.metrics.distance import jaccard_distance
from nltk.util import ngrams
from nltk.metrics.distance  import edit_distance

def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):
    
    output = []
    # Calculate the distance metric 
    for entry in entries:
        temp = [(jaccard_distance(set(ngrams(entry, 3)), set(ngrams(w, 3))),w) for w in correct_spellings if w[0]==entry[0]]
        #print(sorted(temp, key = lambda val:val[0])[0][1])
        output.append(sorted(temp, key = lambda val:val[0])[0][1])
    return output # Your answer here
    
answer_nine()


Question 10
For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:
Jaccard distance on the 4-grams of the two words.
This function should return a list of length three: ['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation'].

def answer_ten(entries=['cormulent', 'incendenece', 'validrate']):
    
    output = []
    # Calculate the distance metric 
    for entry in entries:
        temp = [(jaccard_distance(set(ngrams(entry, 4)), set(ngrams(w, 4))),w) for w in correct_spellings if w[0]==entry[0]]
        #print(sorted(temp, key = lambda val:val[0])[0][1])
        output.append(sorted(temp, key = lambda val:val[0])[0][1])
    return output # Your answer here
    
answer_ten()


Question 11
For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:
Edit distance on the two words with transpositions.
This function should return a list of length three: ['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation'].


def answer_eleven(entries=['cormulent', 'incendenece', 'validrate']):
    
    output = []
    
    for entry in entries:
        temp = [(edit_distance(entry,w),w) for w in correct_spellings if w[0]==entry[0]]
        #print(sorted(temp, key = lambda val:val[0])[0][1])
        output.append(sorted(temp, key = lambda val:val[0])[0][1])
    return output # Your answer here

answer_eleven()